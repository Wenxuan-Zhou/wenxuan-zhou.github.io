<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Wenxuan Zhou</title>
  
  <meta name="author" content="Wenxuan Zhou">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Wenxuan Zhou</name>
              </p>
              <p>I am a Ph.D. student at the Robotics Institute (RI) at Carnegie Mellon University,
                advised by <a href="https://davheld.github.io/">Prof. David Held</a>.
                I am also a Visiting Researcher at FAIR Pittsburgh collaborating with <a href="https://cpaxton.github.io/about/">Chris Paxton</a>.
                My research goal is to equip robots with complex and intelligent behaviors with reinforcement learning.
              </p>
              <p> I interned at <a href="https://deepmind.com/">DeepMind</a> as a Research Scientist Intern with Team REAL and the robotics team during Summer 2021. 
                I received my master's degree at RI mentored by <a href="https://cs.nyu.edu/~lp91/">Lerrel Pinto</a>
                and advised by <a href="http://www.cs.cmu.edu/~abhinavg/">Prof. Abhinav Gupta</a>.
                During my undergraduate study, I worked with <a href="http://www-personal.umich.edu/~orosz/">Prof. Gabor Orosz</a>
                on ground robot experiments with connected cruise control.
                I've also interned at ZF TRW at the brake control systems group.</p>

              <p style="text-align:center">
                <a href="https://www.cs.cmu.edu/directory/wenxuanz">Contact</a> &nbsp/&nbsp
                <a href="https://github.com/Wenxuan-Zhou">GitHub</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=picvdvEAAAAJ&hl=en&oi=ao">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/Wenxuan-circle.png" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/og.gif" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Learning to Grasp the Ungraspable with Emergent Extrinsic Dexterity</papertitle>
              <br>
              <strong>Wenxuan Zhou</strong>, <a href="http://davheld.github.io/">David Held</a>
              <br>
              <em>Conference of Robot Learning 2022 <span style="color:#FF0000">(Oral)</span></em>
              <br>
              <em>ICRA 2022 Workshop on Reinforcement Learning for Contact-Rich Manipulation</em>
              <br>
              <em>Press Coverage: IEEE Specturm - <a href="https://spectrum.ieee.org/robot-gripper-extrinsic-dexterity">Robots Grip Better When They Grip Smarter</a></em>
              <p> We present a system that applies reinforcement learning to extrinsic dexterity that solves an occluded grasping task with a simple gripper.</p>
              <p>#Manipulation #Sim2Real</p>
              <a href="https://arxiv.org/abs/2211.01500">[Paper]</a>
              <a href="https://github.com/Wenxuan-Zhou/ungraspable">[Code]</a>
              <a href="https://sites.google.com/view/grasp-ungraspable">[Website]</a>
              <br>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/od.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Forgetting and Imbalance in Robot Lifelong Learning with Off-policy Data
              </papertitle>
              <br>
              <strong>Wenxuan Zhou</strong>, Steven Bohez, Jan Humplik, Abbas Abdolmaleki, Dushyant Rao, Markus Wulfmeier, Tuomas Haarnoja, Nicolas Heess
              <br>
              <em>Conference on Lifelong Learning Agents (CoLLAs) 2022</em>
              <br>
              <p> We identify two challenges in robot lifelong learning with non-stationary dynamics due to off-policy data.</p>
              <p>#Lifelong_Learning #Offline_RL #Off_Policy_RL</p>
              <a href="https://arxiv.org/abs/2204.05893">[Paper]</a>
              <a href="https://virtual.lifelong-ml.cc/poster_61.html">[Website]</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/loop-v2.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Learning Off-Policy with Online Planning</papertitle>
              <br>
              <a href="https://hari-sikchi.github.io/">Harshit Sikchi</a>, <strong>Wenxuan Zhou</strong>, <a href="http://davheld.github.io/">David Held</a>
              <br>
              <em>Conference of Robot Learning 2021 <span style="color:#FF0000">(Oral, Best Paper Finalist)</span></em>
              <p> A novel instantiation of H-step lookahead policies with a learned model and a terminal value from a model-free off-policy algorithm.</p>
              <p>#Model_Based_RL #Model_Free_RL</p>
              <a href="https://arxiv.org/abs/2008.10066">[Paper]</a>
              <a href="https://github.com/hari-sikchi/LOOP">[Code]</a>
              <a href="https://hari-sikchi.github.io/loop/">[Website]</a>
            </td>
          </tr>

           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/LP.gif" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>PLAS: Latent Action Space for Offline Reinforcement Learning</papertitle><br>
              <strong>Wenxuan Zhou</strong>, <a href="https://sujaybajracharya.me/">Sujay Bajracharya</a>, <a href="http://davheld.github.io/">David Held</a><br>
              <em>Conference of Robot Learning 2020 <span style="color:#FF0000">(Plenary Talk)</span></em><br>
              <p>Learning policy in the latent action space to naturally avoid out-of-distribution actions.</p>
              <p>#Offline_RL #Off_Policy_RL #Cloth_Manipulation</p>
              <a href="https://arxiv.org/abs/2011.07213">[Paper]</a>
              <a href="https://github.com/Wenxuan-Zhou/PLAS">[Code]</a>
              <a href="https://sites.google.com/view/latent-policy">[Website]</a><br>
            </td>
          </tr>

           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/LBPO.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Lyapunov Barrier Policy Optimization</papertitle><br>
              <a href="https://hari-sikchi.github.io/">Harshit Sikchi</a>, <strong>Wenxuan Zhou</strong>, <a href="http://davheld.github.io/">David Held</a><br>
              <em>NeurIPS Deep RL Workshop 2020</em><br>
              <p>Safe reinforcement learning with a Lyapunov-based barrier function.</p>
              <p>#Safe_RL</p>
              <a href="https://arxiv.org/abs/2103.09230">[Paper]</a>
              <a href="https://github.com/hari-sikchi/LBPO">[Code]</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/EPI.gif" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>EPI: Environment Probing Interaction Policies</papertitle>
              <br>
              <strong>Wenxuan Zhou</strong>, <a href="https://cs.nyu.edu/~lp91/">Lerrel Pinto</a>, <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a><br>
              <em>ICLR 2019</em><br>
              <p>Learning to "probe" the environment before task execution.</p>
              <p>#System_Identification  #Environment_Generalization  #Multi_Task_Learning</p>
              <a href="https://openreview.net/pdf?id=ryl8-3AcFX">[Paper]</a><a href="https://github.com/Wenxuan-Zhou/EPI">[Code]</a>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Last update: Jan 2023
                <br>
                <a href="https://github.com/jonbarron/website">Source</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
